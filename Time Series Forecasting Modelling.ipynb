{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6808dd9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_csv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# for data Wrrangling\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# metrics\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sqrt\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# matplot lib\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# import functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "# for data Wrrangling\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# matplot lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "\n",
    "#seaborn plot \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# pickle to save file\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02920a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales_train_evaluation.csv')\n",
    "sales.name = 'sales'\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "calendar.name = 'calendar'\n",
    "prices = pd.read_csv('sell_prices.csv')\n",
    "prices.name = 'prices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08de1af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downcast in order to save memory\n",
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df  \n",
    "\n",
    "sales = downcast(sales)\n",
    "prices = downcast(prices)\n",
    "calendar = downcast(calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6c4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add zero sales for the remaining days 1942-1969\n",
    "for d in range(1942,1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales[col] = 0\n",
    "    sales[col] = sales[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f212abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a867299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb79ed4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# label encoding \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Store the categories along with their codes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m d_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[38;5;241m.\u001b[39mcodes, df\u001b[38;5;241m.\u001b[39mid))\n\u001b[1;32m      4\u001b[0m d_item_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df\u001b[38;5;241m.\u001b[39mitem_id\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes, df\u001b[38;5;241m.\u001b[39mitem_id))\n\u001b[1;32m      5\u001b[0m d_dept_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df\u001b[38;5;241m.\u001b[39mdept_id\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes, df\u001b[38;5;241m.\u001b[39mdept_id))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XGBoost/lib/python3.9/site-packages/pandas/core/generic.py:5461\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5454\u001b[0m \u001b[38;5;66;03m# Note: obj.x will always call obj.__getattribute__('x') prior to\u001b[39;00m\n\u001b[1;32m   5455\u001b[0m \u001b[38;5;66;03m# calling obj.__getattr__('x').\u001b[39;00m\n\u001b[1;32m   5456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5457\u001b[0m     name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5458\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5459\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5460\u001b[0m ):\n\u001b[0;32m-> 5461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XGBoost/lib/python3.9/site-packages/pandas/core/accessor.py:180\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 180\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XGBoost/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2457\u001b[0m, in \u001b[0;36mCategoricalAccessor.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m-> 2457\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m   2459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XGBoost/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2466\u001b[0m, in \u001b[0;36mCategoricalAccessor._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   2464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate\u001b[39m(data):\n\u001b[1;32m   2465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_categorical_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m-> 2466\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .cat accessor with a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "# label encoding \n",
    "#Store the categories along with their codes\n",
    "d_id = dict(zip(df.id.cat.codes, df.id))\n",
    "d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\n",
    "d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\n",
    "d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\n",
    "d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\n",
    "d_state_id = dict(zip(df.state_id.cat.codes, df.state_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b68e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#del group, group_price_cat, group_price_store, group_state, group_state_store, cal_data\n",
    "#gc.collect();\n",
    "\n",
    "#2\n",
    "df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "cols = df.dtypes.index.tolist()\n",
    "types = df.dtypes.values.tolist()\n",
    "for i,type in enumerate(types):\n",
    "    if type.name == 'category':\n",
    "        df[cols[i]] = df[cols[i]].cat.codes\n",
    "        \n",
    "#3\n",
    "df.drop('date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b3ea22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduce lags\n",
    "lags = [1,2,3,6,12,24,36]\n",
    "for lag in lags:\n",
    "    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d79216b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sold</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>sold_lag_1</th>\n",
       "      <th>sold_lag_2</th>\n",
       "      <th>sold_lag_3</th>\n",
       "      <th>sold_lag_6</th>\n",
       "      <th>sold_lag_12</th>\n",
       "      <th>sold_lag_24</th>\n",
       "      <th>sold_lag_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60034805</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_823</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60034806</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_824</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60034807</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_825</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60034808</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_826</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60034809</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60034810 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id        item_id    dept_id   cat_id  \\\n",
       "0         HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "1         HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2         HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "3         HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "4         HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                                 ...            ...        ...      ...   \n",
       "60034805    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n",
       "60034806    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n",
       "60034807    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n",
       "60034808    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n",
       "60034809    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n",
       "\n",
       "         store_id state_id     d  sold  wm_yr_wk   weekday  ...  snap_TX  \\\n",
       "0            CA_1       CA     1     0     11101  Saturday  ...        0   \n",
       "1            CA_1       CA     1     0     11101  Saturday  ...        0   \n",
       "2            CA_1       CA     1     0     11101  Saturday  ...        0   \n",
       "3            CA_1       CA     1     0     11101  Saturday  ...        0   \n",
       "4            CA_1       CA     1     0     11101  Saturday  ...        0   \n",
       "...           ...      ...   ...   ...       ...       ...  ...      ...   \n",
       "60034805     WI_3       WI  1969     0     11621    Sunday  ...        0   \n",
       "60034806     WI_3       WI  1969     0     11621    Sunday  ...        0   \n",
       "60034807     WI_3       WI  1969     0     11621    Sunday  ...        0   \n",
       "60034808     WI_3       WI  1969     0     11621    Sunday  ...        0   \n",
       "60034809     WI_3       WI  1969     0     11621    Sunday  ...        0   \n",
       "\n",
       "          snap_WI  sell_price sold_lag_1 sold_lag_2 sold_lag_3 sold_lag_6  \\\n",
       "0               0         NaN        NaN        NaN        NaN        NaN   \n",
       "1               0         NaN        NaN        NaN        NaN        NaN   \n",
       "2               0         NaN        NaN        NaN        NaN        NaN   \n",
       "3               0         NaN        NaN        NaN        NaN        NaN   \n",
       "4               0         NaN        NaN        NaN        NaN        NaN   \n",
       "...           ...         ...        ...        ...        ...        ...   \n",
       "60034805        0        2.98        0.0        0.0        0.0        0.0   \n",
       "60034806        0        2.48        0.0        0.0        0.0        0.0   \n",
       "60034807        0        3.98        0.0        0.0        0.0        0.0   \n",
       "60034808        0        1.28        0.0        0.0        0.0        0.0   \n",
       "60034809        0        1.00        0.0        0.0        0.0        0.0   \n",
       "\n",
       "          sold_lag_12  sold_lag_24  sold_lag_36  \n",
       "0                 NaN          NaN          NaN  \n",
       "1                 NaN          NaN          NaN  \n",
       "2                 NaN          NaN          NaN  \n",
       "3                 NaN          NaN          NaN  \n",
       "4                 NaN          NaN          NaN  \n",
       "...               ...          ...          ...  \n",
       "60034805          0.0          0.0          0.0  \n",
       "60034806          0.0          0.0          0.0  \n",
       "60034807          0.0          0.0          0.0  \n",
       "60034808          0.0          0.0          1.0  \n",
       "60034809          0.0          0.0          2.0  \n",
       "\n",
       "[60034810 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b56543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a13a1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d46ef834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "223193d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\n",
    "df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\n",
    "df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4c6271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['d']>=36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e709143",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f846c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_pickle('data.pkl')\n",
    "del df\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')\n",
    "valid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\n",
    "test = data[data['d']>=1942][['id','d','sold']]\n",
    "eval_preds = test['sold']\n",
    "valid_preds = valid['sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde06257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the store ids\n",
    "stores = sales.store_id.cat.codes.unique().tolist()\n",
    "for store in stores:\n",
    "    df = data[data['store_id']==store]\n",
    "    \n",
    "    #Split the data\n",
    "    X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\n",
    "    X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\n",
    "    X_test = df[df['d']>=1942].drop('sold',axis=1)\n",
    "    \n",
    "    #Train and validate\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        max_depth=8,\n",
    "        num_leaves=50,\n",
    "        min_child_weight=300\n",
    "    )\n",
    "    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n",
    "             eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n",
    "    valid_preds[X_valid.index] = model.predict(X_valid)\n",
    "    eval_preds[X_test.index] = model.predict(X_test)\n",
    "    filename = 'model'+str(d_store_id[store])+'.pkl'\n",
    "    # save model\n",
    "    joblib.dump(model, filename)\n",
    "    del model, X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
